# llm_crawler

# ğŸ‘‹Introduction

Web scraper is a Python-based tool designed to extract data from web pages and APIs. With its powerful data extraction capabilities, our scraper can automate the collection of large amounts of data from various sources, including e-commerce sites, social media platforms, and news outlets.

In this project, we use two web crawlers to scrape high-quality Chinese articles (including Simplified and Traditional Chinese) for training Chinese large language models (LLMs) in general.

## **Installation**

```
pip install -r requirements.txt
```
# ğŸšŒ**Usage**
## **Requests**
### **01 Working principle**
Coding process of requests module:
- Specify url
    - UA camouflage
    - Processing of request parameters
- Initiate a request
- Get response data
- Persistent storage

![Alt text](image-20210803105808636.png)


### **02 Examples**
#### Daily news web pageï¼ˆhttp://mrxwlb.com/category/mrxwlb-text/ï¼‰
**Input**
```python
import requests
from lxml import etree
import json

urls = [f'http://mrxwlb.com/category/mrxwlb-text/page/{i}/' for i in range(1,3)]
headers = {
    'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}
data = []
for url in urls:
    resp = requests.get(url=url, headers=headers)
    html = etree.HTML(resp.text)
    list = html.xpath('//header[@class="entry-header"]/h1')
    for a in list:
        title = a.xpath('./a/text()')[0].strip()
        page_url = a.xpath('./a/@href')[0]
        page_resp = requests.get(url=page_url, headers=headers)
        page_html = etree.HTML(page_resp.text)
        page_list = page_html.xpath('//section[@class="entry-content"]/p')
        text = ''
        for p in page_list:
            if p.text is not None:
                text += p.text
        data.append({"title": title, "text": text})

with open("data.json", 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)
```
**Output**
```
[
  {
    "title": "2023å¹´07æœˆ03æ—¥æ–°é—»è”æ’­æ–‡å­—ç‰ˆ",
    "text": "7æœˆ3æ—¥ï¼Œä¸­å…±ä¸­å¤®æ€»ä¹¦è®°ã€å›½å®¶ä¸»å¸­ä¹ ..."
  },
  {
    "title": "2023å¹´07æœˆ02æ—¥æ–°é—»è”æ’­æ–‡å­—ç‰ˆ",
    "text": "åœ¨å…¨é¢è´¯å½»å…šçš„äºŒåå¤§ç²¾ç¥çš„å¼€å±€ä¹‹å¹´ï¼Œæˆ‘ä»¬è¿æ¥ä¸­å›½å…±äº§å…šæˆç«‹102å‘¨å¹´åè¯ã€‚å†å²è§è¯å£®é˜”çš„è¡Œè¿›..."
  },
  {
    "title": "2023å¹´07æœˆ01æ—¥æ–°é—»è”æ’­æ–‡å­—ç‰ˆ",
    "text": "ä¸­å…±ä¸­å¤®æ”¿æ²»å±€6æœˆ30æ—¥ä¸‹åˆ..."
  },
  {
    "title": "2023å¹´06æœˆ30æ—¥æ–°é—»è”æ’­æ–‡å­—ç‰ˆ",
    "text": "ä¸­å…±ä¸­å¤®æ”¿æ²»å±€..."
  },
  ...
]
```

## **Scrapy**
Scrapy remains by far the most popular crawler framework. Here is a part of official introduction to scrapy.

```
An open source and collaborative framework for extracting the data you need from websites.

In a fast, simple, yet extensible way.
```

The features of scrapy are: high speed, simplicity and scalability.

Scrapy official documentation: https://docs.scrapy.org/en/latest/

### **01 Working principle**

![Alt text](image-20210803111853028.png)


The whole work flow:

1. The url starting in the crawler is constructed as a request object and passed to the scheduler.

2. The `engine` gets the request object from the `scheduler`. Then give it to the `downloader`.

3. The source code of the page is obtained by the `downloader` and packaged into a response object. And feed back to the `engine`.

4. The `engine` passes the response object to the `spider`, which parses the data (parse). And feed back to the `engine`.

5. `Engine` will pass data to pipeline for data persistence or further data processing.

6. During this period, if the spider is not extracting data. Instead, the subpage url. Can be further submitted to the scheduler to repeat the process of `Step 2`


### **02 Examples**

#### 01--xianfaï¼ˆå®ªæ³•ï¼‰
1. Start a scrapy project.
    ```
    scrapy startproject xianfa
    ```
2. Download the uploaded Xianfa.py file in the `path\xianfa\xianfa\spiders` folder or create the Xianfa.py file and copy and paste the following code into it.

3. Transfer path to `path\xianfa\xianfa\spiders`.
    ```
    d:> cd d:\path\xianfa\xianfa\spiders

    d:\path\xianfa\xianfa\spiders> scrapy crawl xianfa -o xianfa.json
    # OR
    d:\path\xianfa\xianfa\spiders> scrapy runspider xianfa.py -o xianfa.json
    ```

4. Crawled content will be saved to the `path\xianfa\xianfa\spiders` directory in json format.

- **Note:** The chapter3 web page is special and requires a second operation, that is, comment the code under "Web crawling content except chapter3", and uncomment the code under "chapter3 crawling content", and then repeat step 3 again.

**Input**
```python
import scrapy
import requests
import json

from xianlaw.items import XianlawItem


class XianfaSpider(scrapy.Spider):
    name = "xianfa"
    allowed_domains = ["www.basiclaw.gov.hk"]
    #use this when crawling others expect for chapter3
    start_urls = [f"https://www.basiclaw.gov.hk/sc/constitution/chapter{i}.html" for i in range(1,5)]

    #use this when crawling chapter3
    #start_urls = ["https://www.basiclaw.gov.hk/sc/constitution/chapter3.html"]

    def parse(self, response):
        #use this when crawling others expect for chapter3
        elements = response.xpath('//body/*')

        #use this when crawling chapter3
        #elements = response.xpath('//div[@class="list_content"]/*')

        text = ''
        title = ''
        for e in elements:
            if e.xpath('self::h2'):
                if text:
                    yield {"text": title + "ï¼š" + text}
                    text = ''
                title = e.xpath('string()').get().strip()
            elif e.xpath('self::p'):
                text += e.xpath('string()').get().strip()
                text = text.replace('\r', '').replace('\n', '')
        if text:
            yield {"text": title +"ï¼š" + text}

    def parse_start_url(self, response):
        # odering
        yield from sorted(super().parse_start_url(response), key=lambda x: int(x["text"].split(".")[0]))    
```
**Output**
```
[
	{"text": "åºè¨€:ä¸­å›½æ˜¯ä¸–ç•Œä¸Šå†å²æœ€æ‚ ä¹…çš„å›½å®¶ä¹‹ä¸€..."},
	{"text": "ç¬¬ä¸€æ¡ï¼šä¸­åäººæ°‘å…±å’Œå›½æ˜¯å·¥äººé˜¶çº§é¢†å¯¼çš„..."},
	{"text": "ç¬¬äºŒæ¡ï¼šä¸­åäººæ°‘å…±å’Œå›½çš„ä¸€åˆ‡æƒåŠ›å±äºäººæ°‘ã€‚..."},
	...
	{"text": "ç¬¬ä¸€ç™¾å››åä¸‰æ¡ï¼šä¸­åäººæ°‘å…±å’Œå›½é¦–éƒ½æ˜¯åŒ—äº¬ã€‚"}
]

```

### **02--basiclawï¼ˆåŸºæœ¬æ³•ï¼‰**

1. Start a scrapy project.
```
scrapy startproject basiclaw
```
2. Download the uploaded basiclaw.py file in the `path\basiclaw\basiclaw\spiders` folder or create a basiclaw.py file and copy and paste the following code into it.

3. Transfer path to `path\basiclaw\basiclaw\spiders `.
```
d:> cd d:\path\basiclaw\basiclaw\spiders

d:\path\basiclaw\basiclaw\spiders> scrapy crawl basiclaw -o basiclaw.json
#OR
d:\path\basiclaw\basiclaw\spiders> scrapy runspider basiclaw.py -o basiclaw.json
```

4. Crawling content is saved as json to the `path\basiclaw\basiclaw\spiders`.
- **Note:** The chapter4,5 page is special and requires a second operation, that is, comment the code that has been marked "Web crawling content except chapter4,5", and uncomment the code that has been marked "chapter4,5 crawling content", and repeat step 3 again

Here is the code.
```python
import scrapy
from xianlaw.items import XianlawItem
import re

class XianfaSpider(scrapy.Spider):
    name = "basiclaw"
    allowed_domains = ["www.basiclaw.gov.hk"]
    url_list = [
        "https://www.basiclaw.gov.hk/sc/basiclaw/decree.html",
                "https://www.basiclaw.gov.hk/sc/basiclaw/basiclaw.html",
                "https://www.basiclaw.gov.hk/sc/basiclaw/preamble.html",
                "https://www.basiclaw.gov.hk/sc/basiclaw/chapter1.html",
                "https://www.basiclaw.gov.hk/sc/basiclaw/chapter2.html",
                "https://www.basiclaw.gov.hk/sc/basiclaw/chapter3.html",
                # "https://www.basiclaw.gov.hk/sc/basiclaw/chapter4.html",
                # "https://www.basiclaw.gov.hk/sc/basiclaw/chapter5.html"
                "https://www.basiclaw.gov.hk/sc/basiclaw/chapter6.html",
                "https://www.basiclaw.gov.hk/sc/basiclaw/chapter7.html",
                "https://www.basiclaw.gov.hk/sc/basiclaw/chapter8.html",
                "https://www.basiclaw.gov.hk/sc/basiclaw/chapter9.html",
                "https://www.basiclaw.gov.hk/sc/basiclaw/annex-instrument.html",
                "https://www.basiclaw.gov.hk/sc/basiclaw/national-laws.html"
                ]
    def start_requests(self):
        for url in self.url_list:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        #use this when crawling other chapters except for chapter4 and 4
        elements = response.xpath('//body/*')
        #use this when crawling chapter4 and 5
        #elements = response.xpath('//div[@class="list_content"]/*')

        text = ''
        title = ''
        for e in elements:
            if e.xpath('self::h2'):
                if text:
                    yield {"text": title + "ï¼š" + text}
                    text = ''
                title = e.xpath('string()').get().strip()
            elif e.xpath('self::p'):
                text += e.xpath('string()').get().strip()
                text = text.replace('\r', '').replace('\n', '')
        if text:
            yield {"text": title +"ï¼š" + text}

    def parse_start_url(self, response):
        # Odering
        yield from sorted(super().parse_start_url(response), key=lambda x: int(x["text"].split(".")[0]))    
```
**Output**
```
[
	{"text": "ä¸­åäººæ°‘å…±å’Œå›½ä¸»å¸­ä»¤ï¼šç¬¬äºŒåå…­å·ã€Šä¸­åäººæ°‘å…±å’Œå›½é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºåŸºæœ¬æ³•ã€‹..."},
	{"text": "ä¸­åäººæ°‘å…±å’Œå›½é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºåŸºæœ¬æ³•ï¼š..."},
	{"text": "åºè¨€ï¼šé¦™æ¸¯è‡ªå¤ä»¥æ¥å°±æ˜¯ä¸­å›½çš„é¢†åœŸ..."},
	...
	{"text": "ç¬¬ä¸€ç™¾äº”åä¹æ¡ï¼šæœ¬æ³•çš„ä¿®æ”¹æƒå±äºå…¨å›½äººæ°‘ä»£è¡¨å¤§ä¼šã€‚..."},
	{"text": "ç¬¬ä¸€ç™¾å…­åæ¡ï¼šé¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºæˆç«‹æ—¶ï¼Œé¦™æ¸¯åŸæœ‰..."}
]
```
---
## scrapyåŸºç¡€æ•™ç¨‹

â€‹	æ¥ä¸‹æ¥, æˆ‘ä»¬ç”¨scrapyæ¥å®Œæˆä¸€ä¸ªè¶…çº§ç®€å•çš„çˆ¬è™«, ç›®æ ‡: æ·±å…¥ç†è§£Scrapyå·¥ä½œçš„æµç¨‹, ä»¥åŠå„ä¸ªæ¨¡å—ä¹‹é—´æ˜¯å¦‚ä½•æ­é…å·¥ä½œçš„. 

1. åˆ›å»ºé¡¹ç›®

    ```
    scrapy startproject é¡¹ç›®åç§°
    ```

    ç¤ºä¾‹:

    ```
    scrapy startproject mySpider_2
    ```

    åˆ›å»ºå¥½é¡¹ç›®å, æˆ‘ä»¬å¯ä»¥åœ¨pycharmé‡Œè§‚å¯Ÿåˆ°scrapyå¸®æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–‡ä»¶å¤¹, é‡Œé¢çš„ç›®å½•ç»“æ„å¦‚ä¸‹:

    ```python
    mySpider_2   # é¡¹ç›®æ‰€åœ¨æ–‡ä»¶å¤¹, å»ºè®®ç”¨pycharmæ‰“å¼€è¯¥æ–‡ä»¶å¤¹
        â”œâ”€â”€ mySpider_2  		# é¡¹ç›®è·Ÿç›®å½•
        â”‚Â Â  â”œâ”€â”€ __init__.py
        â”‚Â Â  â”œâ”€â”€ items.py  		# å°è£…æ•°æ®çš„æ ¼å¼
        â”‚Â Â  â”œâ”€â”€ middlewares.py  # æ‰€æœ‰ä¸­é—´ä»¶
        â”‚Â Â  â”œâ”€â”€ pipelines.py	# æ‰€æœ‰çš„ç®¡é“
        â”‚Â Â  â”œâ”€â”€ settings.py		# çˆ¬è™«é…ç½®ä¿¡æ¯
        â”‚Â Â  â””â”€â”€ spiders			# çˆ¬è™«æ–‡ä»¶å¤¹, ç¨åé‡Œé¢ä¼šå†™å…¥çˆ¬è™«ä»£ç 
        â”‚Â Â      â””â”€â”€ __init__.py
        â””â”€â”€ scrapy.cfg			# scrapyé¡¹ç›®é…ç½®ä¿¡æ¯,ä¸è¦åˆ å®ƒ,åˆ«åŠ¨å®ƒ,å–„å¾…å®ƒ. 
    
    ```

2. åˆ›å»ºçˆ¬è™«

    ```python
    cd æ–‡ä»¶å¤¹  # è¿›å…¥é¡¹ç›®æ‰€åœ¨æ–‡ä»¶å¤¹
    scrapy genspider çˆ¬è™«åç§° å…è®¸æŠ“å–çš„åŸŸåèŒƒå›´
    ```

    ç¤ºä¾‹:

    ```
    cd mySpider_2
    scrapy genspider youxi 4399.com
    ```

    æ•ˆæœ:

    ```python
    (base) sylardeMBP:ç¬¬ä¸ƒç«  sylar$ cd mySpider_2
    (base) sylardeMBP:mySpider_2 sylar$ ls
    mySpider_2      scrapy.cfg
    (base) sylardeMBP:mySpider_2 sylar$ scrapy genspider youxi http://www.4399.com/
    Created spider 'youxi' using template 'basic' in module:
      mySpider_2.spiders.youxi
    (base) sylardeMBP:mySpider_2 sylar$ 
    ```

    è‡³æ­¤, çˆ¬è™«åˆ›å»ºå®Œæ¯•, æˆ‘ä»¬æ‰“å¼€æ–‡ä»¶å¤¹çœ‹ä¸€ä¸‹. 

    ```python
    â”œâ”€â”€ mySpider_2
    â”‚Â Â  â”œâ”€â”€ __init__.py
    â”‚Â Â  â”œâ”€â”€ items.py
    â”‚Â Â  â”œâ”€â”€ middlewares.py
    â”‚Â Â  â”œâ”€â”€ pipelines.py
    â”‚Â Â  â”œâ”€â”€ settings.py
    â”‚Â Â  â””â”€â”€ spiders
    â”‚Â Â      â”œâ”€â”€ __init__.py
    â”‚Â Â      â””â”€â”€ youxi.py   # å¤šäº†ä¸€ä¸ªè¿™ä¸ª. 
    â””â”€â”€ scrapy.cfg
    
    ```

    

3. ç¼–å†™æ•°æ®è§£æè¿‡ç¨‹

    å®Œå–„youxi.pyä¸­çš„å†…å®¹. 

    ```python
    import scrapy
    
    class YouxiSpider(scrapy.Spider):
        name = 'youxi'  # è¯¥åå­—éå¸¸å…³é”®, æˆ‘ä»¬åœ¨å¯åŠ¨è¯¥çˆ¬è™«çš„æ—¶å€™éœ€è¦è¿™ä¸ªåå­—
        allowed_domains = ['4399.com']  # çˆ¬è™«æŠ“å–çš„åŸŸ.
        start_urls = ['http://www.4399.com/flash/']  # èµ·å§‹é¡µ
    
        def parse(self, response, **kwargs):
            # response.text  # é¡µé¢æºä»£ç 
            # response.xpath()  # é€šè¿‡xpathæ–¹å¼æå–
            # response.css()  # é€šè¿‡cssæ–¹å¼æå–
            # response.json() # æå–jsonæ•°æ®
    
            # ç”¨æˆ‘ä»¬æœ€ç†Ÿæ‚‰çš„æ–¹å¼: xpathæå–æ¸¸æˆåç§°, æ¸¸æˆç±»åˆ«, å‘å¸ƒæ—¶é—´ç­‰ä¿¡æ¯
            li_list = response.xpath("//ul[@class='n-game cf']/li")
            for li in li_list:
                name = li.xpath("./a/b/text()").extract_first()
                category = li.xpath("./em/a/text()").extract_first()
                date = li.xpath("./em/text()").extract_first()
    
                dic = {
                    "name": name,
                    "category": category,
                    "date": date
                }
    
                # å°†æå–åˆ°çš„æ•°æ®æäº¤åˆ°ç®¡é“å†….
                # æ³¨æ„, è¿™é‡Œåªèƒ½è¿”å› requestå¯¹è±¡, å­—å…¸, itemæ•°æ®, or None
                yield dic
    
    ```

    æ³¨æ„: 

    ==spiderè¿”å›çš„å†…å®¹åªèƒ½æ˜¯å­—å…¸, requesteså¯¹è±¡, itemæ•°æ®æˆ–è€…None. å…¶ä»–å†…å®¹ä¸€å¾‹æŠ¥é”™==

    è¿è¡Œçˆ¬è™«: 

    ```cmd
    scrapy crawl çˆ¬è™«åå­—
    ```

    å®ä¾‹:

    ```
    scrapy crawl youxi
    ```

    

4. ç¼–å†™pipeline.å¯¹æ•°æ®è¿›è¡Œç®€å•çš„ä¿å­˜

    æ•°æ®ä¼ é€’åˆ°pipeline, æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹åœ¨pipelineä¸­çš„æ ·å­. 

    é¦–å…ˆä¿®æ”¹settings.pyæ–‡ä»¶ä¸­çš„pipelineä¿¡æ¯

    ```python
    ITEM_PIPELINES = {
        # å‰é¢æ˜¯pipelineçš„ç±»ååœ°å€               
        # åé¢æ˜¯ä¼˜å…ˆçº§, ä¼˜å…ˆçº§æœˆä½è¶Šå…ˆæ‰§è¡Œ
       'mySpider_2.pipelines.Myspider2Pipeline': 300,
    }
    ```

    ç„¶åæˆ‘ä»¬ä¿®æ”¹ä¸€ä¸‹pipelineä¸­çš„ä»£ç :

    ```python
    class Myspider2Pipeline:
        # è¿™ä¸ªæ–¹æ³•çš„å£°æ˜ä¸èƒ½åŠ¨!!! åœ¨spiderè¿”å›çš„æ•°æ®ä¼šè‡ªåŠ¨çš„è°ƒç”¨è¿™é‡Œçš„process_itemæ–¹æ³•. 
        # ä½ æŠŠå®ƒæ”¹äº†. ç®¡é“å°±æ–­äº†
        def process_item(self, item, spider):
            print(item)
            return item
    ```

    

## è‡ªå®šä¹‰æ•°æ®ä¼ è¾“ç»“æ„item

â€‹		åœ¨ä¸Šè¿°æ¡ˆä¾‹ä¸­, æˆ‘ä»¬ä½¿ç”¨å­—å…¸ä½œä¸ºæ•°æ®ä¼ é€’çš„è½½ä½“, ä½†æ˜¯å¦‚æœæ•°æ®é‡éå¸¸å¤§. ç”±äºå­—å…¸çš„keyæ˜¯éšæ„åˆ›å»ºçš„. ææ˜“å‡ºç°é—®é¢˜,  æ­¤æ—¶å†ç”¨å­—å…¸å°±ä¸åˆé€‚äº†. Scrapyä¸­æä¾›itemä½œä¸ºæ•°æ®æ ¼å¼çš„å£°æ˜ä½ç½®. æˆ‘ä»¬å¯ä»¥åœ¨items.pyæ–‡ä»¶æå‰å®šä¹‰å¥½è¯¥çˆ¬è™«åœ¨è¿›è¡Œæ•°æ®ä¼ è¾“æ—¶çš„æ•°æ®æ ¼å¼. ç„¶åå†å†™ä»£ç çš„æ—¶å€™å°±æœ‰äº†æ•°æ®åç§°çš„ä¾æ®äº†. 

item.pyæ–‡ä»¶

```python
import scrapy

class GameItem(scrapy.Item):
    # å®šä¹‰æ•°æ®ç»“æ„
    name = scrapy.Field()
    category = scrapy.Field()
    date = scrapy.Field()
```

spiderä¸­. è¿™æ ·æ¥ä½¿ç”¨:

```python
from mySpider_2.items import GameItem

# ä»¥ä¸‹ä»£ç åœ¨spiderä¸­çš„parseæ›¿æ¢æ‰åŸæ¥çš„å­—å…¸
item = GameItem()
item["name"] = name
item["category"] = category
item["date"] = date
yield item
```



## scrapyä½¿ç”¨å°æ€»ç»“

è‡³æ­¤, æˆ‘ä»¬å¯¹scrapyæœ‰äº†ä¸€ä¸ªéå¸¸åˆæ­¥çš„äº†è§£å’Œä½¿ç”¨. å¿«é€Ÿæ€»ç»“ä¸€ä¸‹. scrapyæ¡†æ¶çš„ä½¿ç”¨æµç¨‹: 

1. åˆ›å»ºçˆ¬è™«é¡¹ç›®.   `scrapy startproject xxx     `
2. è¿›å…¥é¡¹ç›®ç›®å½•.    `cd xxx  `
3. åˆ›å»ºçˆ¬è™«            `scrapy genspider åç§° æŠ“å–åŸŸ`
4. ç¼–å†™`item.py` æ–‡ä»¶, å®šä¹‰å¥½æ•°æ®item
5. ä¿®æ”¹spiderä¸­çš„parseæ–¹æ³•. å¯¹è¿”å›çš„å“åº”responseå¯¹è±¡è¿›è¡Œè§£æ. è¿”å›item
6. åœ¨pipelineä¸­å¯¹æ•°æ®è¿›è¡Œä¿å­˜å·¥ä½œ. 
7. ä¿®æ”¹`settings.py`æ–‡ä»¶, å°†pipelineè®¾ç½®ä¸ºç”Ÿæ•ˆ, å¹¶è®¾ç½®å¥½ä¼˜å…ˆçº§
8. å¯åŠ¨çˆ¬è™«   `scrapy crawl åç§°`

